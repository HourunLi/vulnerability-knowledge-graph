from dataGen import *
from evaluate import *
import torch.nn.functional as F

def calculate(h, r, t):
    score = (h + r) - t
    score = torch.norm(score, p=1, dim=-1)
    # print(score.shape)
    return score

def getTrainTriple(triple_dict, train_data_loader):
    for batch_idx, data_batch in enumerate(train_data_loader):
        head, descption, rel, tail, triple_list= data_batch
        for ind in range(len(triple_list)):
            h, t = triple_list[ind][0].item(), triple_list[ind][2].item()
            if h not in triple_dict:
                triple_dict[h] = set()
            triple_dict[h].add(t)
    # print(triple_dict)
    return triple_dict

def valid_entity_embeddings(head, rel, tail, triple_list, ent_embeddings, triple_dict):
    # rel_embeddings = np.loadtxt("relation_embeddings.txt")
    # canpre_embed = rel_embeddings[1]
    ent_embeddings = torch.FloatTensor(ent_embeddings)
    # canpre_embed = torch.FloatTensor(canpre_embed)

    ind = 0
    mrr , mr = .0, .0
    hit1, hit3, hit10 = .0, .0, .0

    mrr_filter, mr_filter = .0, .0
    hit1_filter, hit3_filter, hit10_filter = .0, .0, .0
    dict_key = triple_dict.keys()
    # print(triple_dict)
    while ind < len(triple_list):
        rank, rank_filter = 0, 0
        h, t = triple_list[ind][0], triple_list[ind][2]
        h, t = h.item(), t.item()
        # h_embeddings = head[ind]
        score = calculate(ent_embeddings[h], rel[ind], ent_embeddings)
        bench = score[t]
        for i in range(len(score)):
            if i == t:
                continue;
            if score[i] < bench:
                rank += 1
                if h in dict_key and i in triple_dict[h]:
                    # print("asddasdasdasdas")
                    continue
                rank_filter += 1
        print("test %d and %d, score is %f, rank is %d, filter rank is %d"%(h, t, bench,rank, rank_filter))

        if rank < 1:
            hit1 += 1
        if rank < 3:
            hit3 += 1
        if rank < 10:
            hit10 += 1

        if rank_filter < 1:
            hit1_filter += 1
        if rank_filter < 3:
            hit3_filter += 1
        if rank_filter < 10:
            hit10_filter += 1
        
        mrr += 1/(rank+1)
        mr += (rank+1)

        mrr_filter += 1/(rank_filter+1)
        mr_filter += (rank_filter+1)

        ind += 1

    hit1 /= len(triple_list)
    hit3 /= len(triple_list)
    hit10 /= len(triple_list)

    hit1_filter /= len(triple_list)
    hit3_filter /= len(triple_list)
    hit10_filter /= len(triple_list)

    mrr /= len(triple_list)
    mr /= len(triple_list)

    mrr_filter /= len(triple_list)
    mr_filter /= len(triple_list)
    print("hit1: %f    hit3: %f    hit10: %f" %(hit1, hit3, hit10))
    print("mrr: %f    mr: %f" %(mrr, mr))

    print("filter: hit1: %f    hit3: %f    hit10: %f" %(hit1_filter, hit3_filter, hit10_filter))
    print("filter: mrr: %f    mr: %f" %(mrr_filter, mr_filter)) 

    return 

ent_embeddings, des_embeddings, rel_embeddings = load_embed("entity_embeddings.txt", "desp_embeddings.txt", "relation_embeddings.txt")
train_dataset = make_dataset("train2id.txt", ent_embeddings, des_embeddings, rel_embeddings)
test_dataset = make_dataset("test2id.txt", ent_embeddings, des_embeddings, rel_embeddings)
valid_dataset = make_dataset("valid2id.txt", ent_embeddings, des_embeddings, rel_embeddings)
train_data_loader = DataLoader(train_dataset, batch_size = 500)
test_data_loader = DataLoader(test_dataset, batch_size = 500)
valid_data_loader = DataLoader(valid_dataset, batch_size = 500)

triple_dict = {}
triple_dict = getTrainTriple(triple_dict, train_data_loader)
triple_dict = getTrainTriple(triple_dict, test_data_loader)
triple_dict = getTrainTriple(triple_dict, valid_data_loader)

for batch_idx, data_batch in enumerate(valid_data_loader):
    head, descption, rel, tail, triple_list = data_batch
    valid_entity_embeddings(head, rel, tail, triple_list, ent_embeddings, triple_dict)