import json
import pandas as pd
import numpy as np 

def sequenceLabeling(tag, slice, label):
    for item in slice:
        begin, end = item[1][0], item[1][1]
        for i in range(begin, end):
            tag[i] = label
    return

"""
 tags: 0 for None, 1 for cause, 2 for consequence, 3 for version, 4 for location, 5 for attacker, 6 for triggering operation
 flag: 0 for prediction, 1 for traning or validation
 prepareTrainingData splits the context into single words.
"""
def prepareTrainingData(file_name, output_file, flag = 1):
    js_output = {"tokens": [], "tags": []}
    with open(file_name, 'r', encoding='utf-8') as f_r, open(output_file, "w", encoding="utf-8") as f_w:
        cnt = 0
        while True:
            cnt += 1
            line = f_r.readline()
            # breaks when meeeting a blank line
            if not line:
                break
            js = json.loads(line)
            context = js["text"]
            context_split = context.split(" ")
            tags = [0] * len(context_split)
            # for ind, ch in enumerate(context_split):
            #     print(ind, ":" + ch)
            if flag:
                labels = js["label"]
                for tag, label_dict in enumerate([labels.get("cause"), labels.get("consequence"), labels.get("version"), labels.get("location"), labels.get("attacker"), labels.get("triggering operation")]):
                    if label_dict is None:
                        continue
                    for _, indexs in label_dict.items():
                        begin, end = indexs[0][0], indexs[0][1]
                        for i in range(begin, end+1):
                            tags[i] = tag + 1

            # Make sure the length of context chars and tags have the same length
            assert len(tags) == len(context_split), "the length of tags doesn't equal to that of context."
            js_output["tokens"] = context_split
            js_output["tags"] = tags
            json.dump(js_output, f_w, ensure_ascii=False)
            f_w.write("\n")
    f_r.close()
    f_w.close()

def processPredictLine(line, js):
    label_list = {1: "cause", 2: "consequence", 3: "version", 4: "location", 5: "attacker", 6: "triggering operation"}
    context, labels = line[0], line[1]
    begin, end = 1, len(context) - 1
    
    pre_cat = ['.', '_', '/', '-', '(']
    post_cat = ['.', '_', '/', '-', ')', ',']

    string = ""
    l = begin
    # use the double pointer method to extract the desired answer: condition, coarse, fine
    while l < end:
        curr_label = labels[l]
        if curr_label <= 0 or curr_label >= 7:
            l += 1
            continue
        r = l
        while labels[r]  == curr_label and r < end:
            r += 1
        tmp_l, tmp_r = l, r
        slice = context[tmp_l: tmp_r]
        ind = 0
        str = []
        while ind < len(slice):
            if ind == 0:
                str.append(slice[ind])
            elif len(slice[ind]) > 2 and slice[ind][0] == '#':
                str[-1] += slice[ind][2:]
            elif str[-1][-1] in pre_cat or slice[ind][0] in post_cat:
                str[-1] += slice[ind]
            else:
                str.append(slice[ind])
            ind += 1
        # print(str)
        string = " ".join(str)
        if len(string) > 4:
            # js["label"][label_list[curr_label]].append([string, [tmp_l-begin, tmp_r-begin]])
            js["label"][label_list[curr_label]].append(string)
        l = r
    return


def getPredictResult(raw_text, tokenizer_predictions_file, output_file):
    with open(raw_text, 'r', encoding='utf-8') as f_r1, open(tokenizer_predictions_file, 'r', encoding='utf-8') as f_r2, open(output_file, "w", encoding="utf-8") as f_w:
        raw_text = f_r1.readline()
        line = f_r2.readline()
        while raw_text and line:
            line = eval(line)
            raw_text = json.loads(raw_text)
            # "None", "cause", "consequence", "version", "location", "attacker", "triggering operation"
            js = {"text":raw_text["text"], "label":{"cause":[], "consequence":[], "version":[], "location":[], "attacker":[], "triggering operation":[]}}
            processPredictLine(line, js)
            json.dump(js, f_w, ensure_ascii=False)
            f_w.write("\n")
            raw_text = f_r1.readline()
            line = f_r2.readline()

def prepareTrainingData_wrap():
    prepareTrainingData(file_name = "./train.json", output_file = "./train_processed.json")
    prepareTrainingData(file_name = "./dev.json", output_file = "./dev_processed.json")
    prepareTrainingData(file_name = "./test.json", output_file = "./test_processed.json", flag=0)
    prepareTrainingData(file_name = "./predict.json", output_file = "./predict_processed.json", flag=0)


def getPredictResult_():
    getPredictResult(raw_text = "./predict.json", tokenizer_predictions_file="../test-ner-base/predictions.txt", output_file="./ner_result.json")


def processDescription():
    # pd.set_option('max_colwidth',500)
    # df = pd.read_csv("all_linkNotNull.csv", index_col= 0)
    df = pd.read_csv("all_CVE_details_output.csv", usecols=[13])
    # print(df.head())
    # print(df.iloc[0:5])
    f = open("predict.json", "w", encoding="utf-8")
    for index, row in df.iterrows():
        # print(row["Summmary"])
        # str = row["Summmary"].split(".")
        # print(str)
        # break
        js = {"id":index, "text":row["Summmary"]}
        json.dump(js, f, ensure_ascii=False)
        f.write("\n")
    f.close()

# def processPredictions():
#     f = open("./test-ner/predictions.txt", "r", encoding="utf-8")
#     f_out = open("./result.txt", "w", encoding = "utf-8")
#     line = f.readline()
#     while line:

#         line = f.readline()
#     f.close()
#     f_out.close()

def main():
    # processDescription()
    # prepareTrainingData_wrap()
    getPredictResult_()


if __name__ == "__main__":
    main()

